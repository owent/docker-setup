# /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
---
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-cilium
  namespace: kube-system
spec:
  valuesContent: |-
    # 如果要使用Multus支持多接口，exclusive 要设置未false
    # exclusive: true
    routingMode: "native" # "native" or "tunnel"
    # enableIPv4Masquerade,enableIPv6Masquerade 和 routingMode 相关
    # 如果 routingMode 是 "tunnel"，则enableIPv4Masquerade,enableIPv6Masquerade 必须开启
    enableIPv4Masquerade: true
    enableIPv6Masquerade: true
    # Enable installation of PodCIDR routes between worker nodes if worker nodes share a common L2 network segment.
    # 如果内部Pod ip对其他节点不可达，主要关闭这个选项
    autoDirectNodeRoutes: true
    # cni:
    #   customConf: true # Migration: Don't install a CNI configuration file
    #   uninstall: false # Migration: Don't remove CNI configuration on shutdown
    endpointRoutes:
      enabled: true
    # IPv6基本配置
    ipv6:
      enabled: true
    # IPv4保持启用（双栈）
    ipv4:
      enabled: true
    # NativeRouting 要求宿主机能够直通这些网段
    #ipv4NativeRoutingCIDR: "10.32.0.0/12"
    #ipv6NativeRoutingCIDR: "fd01:0:1:0a20::1/96"
    # 如果使用 eni 模式，ipam.mode 也要配置为 "eni" 。ENI模式需要平台支持, AWS 下推荐eni模式
    # eni:
    #   enabled: true
    ipam:
      mode: "cluster-pool"
      operator:
        # 单个Node的Pod数默认限制为 110 , 但是可以改大。太大也没啥意义
        clusterPoolIPv4MaskSize: 22
        clusterPoolIPv6MaskSize: 116
        clusterPoolIPv4PodCIDRList:
          - "10.32.0.0/16"
        clusterPoolIPv6PodCIDRList:
          - "fd01:0:1:0a20:20::/96"

    # 关键配置：不强制要求IPv6 PodCIDR
    k8s:
      requireIPv6PodCIDR: false

    hubble:
      enabled: true
      relay:
        enabled: true
      ui:
        enabled: true

    # 替代卸载kube-proxy (https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#kube-proxy-hybrid-modes)
    ## kubectl -n kube-system delete ds kube-proxy
    ## # Delete the configmap as well to avoid kube-proxy being reinstalled during a Kubeadm upgrade (works only for K8s 1.19 and newer)
    ## kubectl -n kube-system delete cm kube-proxy
    ## # Run on each node with root permissions:
    ## iptables-save | grep -v KUBE | iptables-restore
    ## # Clear ipvs
    ## ipvsadm -C
    ## # 重启后验证ipvs和iptables不会再自动创建
    ## iptables-save | grep KUBE
    ## ipvsadm -ln
    k8sServiceHost: "auto"
    #k8sServicePort: 6443
    kubeProxyReplacement: true
    socketLB:
      enabled: true
    nodePort:
      enabled: true
      enableHealthCheck: false
      # range: "30000,32767"
    externalIPs:
      enabled: true
    hostPort:
      enabled: true
    ingressController:
      enabled: true
      # -- Set cilium ingress controller to be the default ingress controller
      # This will let cilium ingress controller route entries without ingress class set
      default: true
    bpf:
      masquerade: true
      tproxy: true
      preallocateMaps: true
    bpfClockProbe: true
    localRedirectPolicy: true
    egressGateway:
      enabled: true
    wellKnownIdentities:
      enabled: true
    loadBalancer:
      # acceleration使用native需要查询网卡是否支持: lspci | grep Ethernet
      # https://docs.cilium.io/en/stable/reference-guides/bpf/progtypes/#xdp-drivers
      acceleration: best-effort
      mode: hybrid

    enableLBIPAM: true
    # defaultLBServiceIPAM: lbipam # lbipam, nodeipam, none
